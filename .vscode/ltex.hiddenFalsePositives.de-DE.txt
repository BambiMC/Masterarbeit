{"rule":"GERMAN_WORD_REPEAT_RULE","sentence":"^\\QDiese Themen sind bereits in Dokumenten wie der Informationssicherheitsrichtlinie der FAU fau_is_richtlinie definiert.\\E$"}
{"rule":"GERMAN_SPELLER_RULE","sentence":"^\\QDiese Themen sind bereits in Dokumenten wie der Informationssicherheitsrichtlinie der FAU fau_is_richtlinie definiert.\\E$"}
{"rule":"GERMAN_SPELLER_RULE","sentence":"^\\QDiese Architektur wurde 2017 durch das Paper \"Attention is All You Need\" von Vaswani et al. \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q eingeführt und hat seitdem zu einer Revolution in der Sprachverarbeitung geführt.\\E$"}
{"rule":"GERMAN_SPELLER_RULE","sentence":"^\\QDiese Architektur wurde 2017 durch das Paper \"Attention is All You Need\" von Vaswani et al. \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q eingeführt und hat seitdem zu einer Revolution in der Sprachverarbeitung geführt.\\E$"}
{"rule":"DE_COMPOUND_COHERENCY","sentence":"^\\QAlternativ könnten auch Cloudmodelle genutzt werden, die sondern entweder direkt auf den Geräten verarbeitet werden oder im RRZE \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q der FAU gespeichert werden.\\E$"}
{"rule":"DE_CASE","sentence":"^\\QAlternativ lässt sich auch ein Modell auch live durch RAG (Retrieval Augmented Generation) oder Prompt Engineering anpassen.\\E$"}
{"rule":"DE_CASE","sentence":"^\\QModell (GGUF-Version) Autor Modell Größe 3B Phi-2 TheBloke/phi-2 1.62 GB 72B Llama mradermacher/Rhea-72b-v0.5-i1 31.04 GB 7B Llama NousResearch/Hermes-2-Pro-Mistral-7B 4.11 GB 2B gemma lmstudio-ai/gemma-2b-it 2.67 GB 70B Llama lmstudio-community/Meta-Llama-3-70B-Instruct 21.14 GB 8B Llama lmstudio-community/Meta-Llama-3-8B-Instruct 4.92 GB 13B Llama TheBloke/Llama-2-13B-chat 10.68 GB 34B Llama TheBloke/OrionStar-Yi-34B-Chat-Llama 14.96 GB 7B Llama TheBloke/Mistral-7B-Instruct-v0.2 4.14 GB Verwendete Modelle für diese Tests\\E$"}
{"rule":"GERMAN_WORD_REPEAT_RULE","sentence":"^\\QMLC LLM unterstützt die folgenden Plattformen und Architekturen 2[4]* Plattform Hardware 2-5 AMD GPU NVIDIA GPU Apple GPU Intel GPU 2[2]* Linux / Win Vulkan, ROCm Vulkan, CUDA N/A Vulkan 2-5 macOS Metal (dGPU) N/A Metal Metal (iGPU) Web Browser WebGPU und WASM iOS / iPadOS Metal auf Apple A-Serie GPU Android OpenCL - Adreno GPU OpenCL - Mali GPU\\E$"}
{"rule":"GERMAN_WORD_REPEAT_RULE","sentence":"^\\QMLC LLM unterstützt die folgenden Plattformen und Architekturen 2[4]* Plattform Hardware 2-5 AMD GPU NVIDIA GPU Apple GPU Intel GPU 2[2]* Linux / Windows Vulkan, ROCm Vulkan, CUDA N/A Vulkan 2-5 macOS Metal (dGPU) N/A Metal Metal (iGPU) Web Browser WebGPU und WASM iOS / iPadOS Metal auf Apple A-Serie GPU Android OpenCL - Adreno GPU OpenCL - Mali GPU\\E$"}
{"rule":"GERMAN_SPELLER_RULE","sentence":"^\\Qlabelsubsec:thematischer-schwerpunkt\\E$"}
{"rule":"DE_AGREEMENT","sentence":"^\\QQLoRA (Quantized Low-Rank Adaptation) basiert auf LoRA (Low-Rank Adaptation) und sind beides Techniken, die die Größe des Modells durch Quantisierung reduzieren, um den Speicherverbrauch weiter zu verringern.\\E$"}
{"rule":"GERMAN_WORD_REPEAT_RULE","sentence":"^\\QMLC LLM unterstützt die folgenden Plattformen und Architekturen 2[4]* Plattform Hardware 2-5 AMD GPU NVIDIA GPU Apple GPU Intel GPU 2[2]* Linux / Windows Vulkan, ROCm Vulkan, CUDA N/A Vulkan 2-5 macOS Metal (dGPU) N/A Metal Metal (iGPU) Webbrowser WebGPU und WASM iOS / iPadOS Metal auf Apple A-Serie GPU Android OpenCL Adreno GPU OpenCL Mali GPU\\E$"}
{"rule":"GERMAN_WORD_REPEAT_BEGINNING_RULE","sentence":"^\\QMLC LLM ist eine Open-Source-Plattform und unter der Apache-2.0 Lizenz verfügbar.\\E$"}
{"rule":"TODO","sentence":"^\\QTODO\\E$"}
{"rule":"GERMAN_WORD_REPEAT_RULE","sentence":"^\\QBF16 support = Ampere GPU mit cuda>=11.0 2080ti 3080 V100 A100 1 GPU grayNaN grayNaN yellow!50523 green!50255 2 GPUs red!50731 yellow!50575 yellow!50530 green!50257 3 GPUs red!50727 yellow!50581 yellow!50-||- green!50-||- 4 GPUs red!50734 yellow!50584 yellow!50-||- green!50-||-\\E$"}
{"rule":"GERMAN_WORD_REPEAT_RULE","sentence":"^\\QMethode/Werkzeug Verbessert Trainingsgeschwindigkeit Optimiert Speichernutzung Wahl der Batchgröße Ja Ja Gradientenakkumulation Nein Ja Gradienten-Checkpointing Nein Ja Training mit gemischter Präzision Ja (Nein) Wahl des Optimierers Ja Ja Datenvorabladung Ja Nein DeepSpeed Zero Nein Ja torch.compile Ja Nein Parameter-Effizientes Finetuning (PEFT) Nein Ja Vergleich von Methoden/Werkzeugen zur Verbesserung der Trainingsgeschwindigkeit und Optimierung der Speichernutzung, Quelle: \\E(?:Dummy|Ina|Jimmy-)[0-9]+$"}
{"rule":"GERMAN_WORD_REPEAT_RULE","sentence":"^\\Q2080ti 3080 V100 A100 1 GPU grayNaN grayNaN yellow!50523 green!50255 2 GPUs red!50731 yellow!50575 yellow!50530 green!50257 3 GPUs red!50727 yellow!50581 yellow!50-||- green!50-||- 4 GPUs red!50734 yellow!50584 yellow!50-||- green!50-||-\\E$"}
